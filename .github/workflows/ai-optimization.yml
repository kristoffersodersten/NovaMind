name: NovaMind AI Code Optimization

on:
  workflow_dispatch:
    inputs:
      optimization_type:
        description: 'Type of optimization'
        required: true
        default: 'file_decomposition'
        type: choice
        options:
        - file_decomposition
        - performance_optimization
        - code_quality_enhancement
        - full_analysis
      target_files:
        description: 'Specific files to optimize (or "auto" for AI detection)'
        required: false
        default: 'auto'
      use_azure_ai:
        description: 'Use Azure OpenAI for advanced analysis'
        required: false
        default: true
        type: boolean

  schedule:
    - cron: '0 2 * * 1'  # Every Monday at 2 AM for weekly optimization

env:
  AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
  AZURE_OPENAI_KEY: ${{ secrets.AZURE_OPENAI_KEY }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  analyze-codebase:
    name: AI-Powered Codebase Analysis
    runs-on: ubuntu-latest
    outputs:
      large-files: ${{ steps.analysis.outputs.large-files }}
      optimization-opportunities: ${{ steps.analysis.outputs.opportunities }}
      complexity-score: ${{ steps.analysis.outputs.complexity }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Python for AI analysis
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install AI analysis tools
      run: |
        pip install openai azure-openai requests json5 lizard radon
        
    - name: Advanced Codebase Analysis
      id: analysis
      run: |
        python << 'EOF'
        import os
        import json
        import subprocess
        from pathlib import Path
        import requests
        
        def analyze_swift_files():
            """Analyze Swift files for optimization opportunities"""
            results = {
                "large_files": [],
                "complex_functions": [],
                "optimization_opportunities": [],
                "performance_issues": []
            }
            
            # Find all Swift files
            swift_files = list(Path('.').rglob('*.swift'))
            
            for file in swift_files:
                if any(exclude in str(file) for exclude in ['.build', 'Carthage', 'Pods']):
                    continue
                    
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        lines = content.split('\n')
                        line_count = len(lines)
                        
                    # Check file size
                    if line_count > 400:
                        results["large_files"].append({
                            "file": str(file),
                            "lines": line_count,
                            "recommendation": "decompose_into_modules"
                        })
                    
                    # Analyze complexity patterns
                    if "class" in content and line_count > 200:
                        class_count = content.count("class ")
                        struct_count = content.count("struct ")
                        if class_count + struct_count > 3:
                            results["optimization_opportunities"].append({
                                "file": str(file),
                                "type": "multiple_types_in_file",
                                "suggestion": "split_into_separate_files"
                            })
                    
                    # Check for performance patterns
                    if "for " in content and "for " in content.count > 2:
                        nested_loops = content.count("for")
                        if nested_loops > 3:
                            results["performance_issues"].append({
                                "file": str(file),
                                "issue": "potential_nested_loops",
                                "suggestion": "consider_functional_programming"
                            })
                            
                except Exception as e:
                    print(f"Error analyzing {file}: {e}")
                    
            return results
        
        # Perform analysis
        analysis_results = analyze_swift_files()
        
        # Output for GitHub Actions
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"large-files={json.dumps(analysis_results['large_files'])}\n")
            f.write(f"opportunities={json.dumps(analysis_results['optimization_opportunities'])}\n")
            f.write(f"complexity={len(analysis_results['large_files']) + len(analysis_results['complex_functions'])}\n")
            
        # Save detailed analysis
        with open('codebase_analysis.json', 'w') as f:
            json.dump(analysis_results, f, indent=2)
            
        print("âœ… Codebase analysis completed")
        print(f"ðŸ“Š Found {len(analysis_results['large_files'])} large files")
        print(f"ðŸŽ¯ Identified {len(analysis_results['optimization_opportunities'])} optimization opportunities")
        EOF
        
    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: codebase-analysis
        path: codebase_analysis.json

  azure-ai-optimization:
    name: Azure AI Code Optimization
    runs-on: ubuntu-latest
    needs: analyze-codebase
    if: github.event.inputs.use_azure_ai == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download analysis results
      uses: actions/download-artifact@v4
      with:
        name: codebase-analysis
        
    - name: Setup Azure AI environment
      run: |
        pip install openai azure-identity azure-openai requests
        
    - name: Azure AI-Powered Optimization
      run: |
        python << 'EOF'
        import json
        import os
        from openai import AzureOpenAI
        
        # Initialize Azure OpenAI client
        client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            api_version="2024-02-01",
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        
        def get_ai_optimization_plan(file_analysis):
            """Get AI-powered optimization recommendations"""
            
            prompt = f"""
            You are a Swift code optimization expert. Analyze this codebase analysis and provide specific, actionable optimization recommendations following Apple's Swift guidelines.
            
            Analysis Data:
            {json.dumps(file_analysis, indent=2)}
            
            Please provide:
            1. Detailed decomposition strategy for large files
            2. Performance optimization recommendations
            3. Architecture improvement suggestions
            4. Specific Swift code patterns to implement
            5. Priority order for optimizations
            
            Focus on:
            - Apple's 400-line file recommendation
            - Type bodies â‰¤ 350 lines
            - Function bodies â‰¤ 50 lines
            - SOLID principles
            - Swift best practices
            - Performance considerations
            
            Provide concrete, implementable solutions.
            """
            
            try:
                response = client.chat.completions.create(
                    model="gpt-4o",  # Use your Azure model deployment name
                    messages=[
                        {"role": "system", "content": "You are an expert Swift developer and code architect specializing in iOS/macOS development and following Apple's guidelines."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=4000,
                    temperature=0.1
                )
                
                return response.choices[0].message.content
                
            except Exception as e:
                print(f"Azure AI Error: {e}")
                return "AI optimization unavailable - falling back to rule-based analysis"
        
        # Load analysis results
        with open('codebase_analysis.json', 'r') as f:
            analysis_data = json.load(f)
        
        # Get AI recommendations
        ai_recommendations = get_ai_optimization_plan(analysis_data)
        
        # Save AI optimization plan
        optimization_plan = {
            "analysis_summary": analysis_data,
            "ai_recommendations": ai_recommendations,
            "implementation_priority": [
                "Decompose largest files first",
                "Extract protocols and types",
                "Optimize performance-critical paths",
                "Refactor complex functions",
                "Improve architecture patterns"
            ]
        }
        
        with open('ai_optimization_plan.json', 'w') as f:
            json.dump(optimization_plan, f, indent=2)
            
        # Create human-readable report
        with open('optimization_report.md', 'w') as f:
            f.write("# NovaMind AI Code Optimization Report\n\n")
            f.write(f"**Generated**: {os.popen('date').read().strip()}\n")
            f.write(f"**Analysis Type**: {os.getenv('GITHUB_EVENT_NAME', 'manual')}\n\n")
            f.write("## Analysis Summary\n\n")
            f.write(f"- **Large Files**: {len(analysis_data.get('large_files', []))}\n")
            f.write(f"- **Optimization Opportunities**: {len(analysis_data.get('optimization_opportunities', []))}\n")
            f.write(f"- **Performance Issues**: {len(analysis_data.get('performance_issues', []))}\n\n")
            f.write("## Azure AI Recommendations\n\n")
            f.write(ai_recommendations)
            f.write("\n\n## Implementation Priority\n\n")
            for i, priority in enumerate(optimization_plan["implementation_priority"], 1):
                f.write(f"{i}. {priority}\n")
                
        print("âœ… Azure AI optimization analysis completed")
        EOF
        
    - name: Upload AI optimization plan
      uses: actions/upload-artifact@v4
      with:
        name: ai-optimization-plan
        path: |
          ai_optimization_plan.json
          optimization_report.md

  auto-decomposition:
    name: Automated Code Decomposition
    runs-on: ubuntu-latest
    needs: [analyze-codebase, azure-ai-optimization]
    if: github.event.inputs.optimization_type == 'file_decomposition' || github.event.inputs.optimization_type == 'full_analysis'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download optimization plan
      uses: actions/download-artifact@v4
      with:
        name: ai-optimization-plan
        
    - name: Setup Swift environment
      run: |
        # Install Swift if needed for analysis
        echo "Swift environment ready for decomposition"
        
    - name: Automated File Decomposition
      run: |
        python << 'EOF'
        import json
        import os
        import re
        from pathlib import Path
        
        def decompose_swift_file(file_path, target_lines=400):
            """Intelligently decompose a Swift file"""
            
            with open(file_path, 'r') as f:
                content = f.read()
                lines = content.split('\n')
                
            if len(lines) <= target_lines:
                return False  # No decomposition needed
                
            # Find natural boundaries
            boundaries = []
            current_line = 0
            
            for i, line in enumerate(lines):
                # Look for MARK comments, class/struct boundaries, extensions
                if re.match(r'^\s*//\s*MARK:', line):
                    boundaries.append(('mark', i, line.strip()))
                elif re.match(r'^\s*(class|struct|enum|protocol|extension)', line):
                    boundaries.append(('type', i, line.strip()))
                elif re.match(r'^\s*func\s+\w+', line):
                    boundaries.append(('function', i, line.strip()))
                    
            # Create decomposition plan
            decomposition_plan = {
                "original_file": str(file_path),
                "original_lines": len(lines),
                "boundaries": boundaries,
                "suggested_splits": []
            }
            
            # Suggest splits based on boundaries
            if len(boundaries) > 1:
                segment_size = len(lines) // max(2, len(boundaries) // 2)
                
                for i in range(0, len(boundaries), 2):
                    if i + 1 < len(boundaries):
                        start = boundaries[i][1]
                        end = boundaries[i + 1][1] if i + 1 < len(boundaries) else len(lines)
                        
                        if end - start > 50:  # Only suggest meaningful splits
                            decomposition_plan["suggested_splits"].append({
                                "start_line": start,
                                "end_line": end,
                                "suggested_filename": f"{Path(file_path).stem}_{boundaries[i][0].title()}.swift",
                                "content_type": boundaries[i][0]
                            })
                            
            return decomposition_plan
        
        # Load analysis results
        with open('ai_optimization_plan.json', 'r') as f:
            optimization_data = json.load(f)
            
        decomposition_plans = []
        
        # Process large files
        for large_file in optimization_data.get('analysis_summary', {}).get('large_files', []):
            file_path = large_file['file']
            if Path(file_path).exists():
                plan = decompose_swift_file(file_path)
                if plan:
                    decomposition_plans.append(plan)
                    
        # Save decomposition plans
        with open('decomposition_plans.json', 'w') as f:
            json.dump(decomposition_plans, f, indent=2)
            
        # Create implementation guide
        with open('decomposition_guide.md', 'w') as f:
            f.write("# Automated Code Decomposition Guide\n\n")
            f.write("## Files to Decompose\n\n")
            
            for plan in decomposition_plans:
                f.write(f"### {plan['original_file']}\n")
                f.write(f"- **Current size**: {plan['original_lines']} lines\n")
                f.write(f"- **Suggested splits**: {len(plan['suggested_splits'])}\n\n")
                
                for split in plan['suggested_splits']:
                    f.write(f"  - **{split['suggested_filename']}**: Lines {split['start_line']}-{split['end_line']} ({split['content_type']})\n")
                f.write("\n")
                
        print(f"âœ… Generated decomposition plans for {len(decomposition_plans)} files")
        EOF
        
    - name: Upload decomposition plans
      uses: actions/upload-artifact@v4
      with:
        name: decomposition-plans
        path: |
          decomposition_plans.json
          decomposition_guide.md

  create-optimization-pr:
    name: Create Optimization Pull Request
    runs-on: ubuntu-latest
    needs: [auto-decomposition]
    if: always() && (needs.auto-decomposition.result == 'success' || needs.azure-ai-optimization.result == 'success')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      
    - name: Setup Git
      run: |
        git config --local user.email "ai-optimizer@novamind.com"
        git config --local user.name "NovaMind AI Optimizer"
        
    - name: Create optimization branch
      run: |
        timestamp=$(date +%Y%m%d-%H%M%S)
        branch_name="ai-optimization-$timestamp"
        git checkout -b "$branch_name"
        echo "BRANCH_NAME=$branch_name" >> $GITHUB_ENV
        
    - name: Organize optimization artifacts
      run: |
        mkdir -p .github/optimization
        
        # Move all optimization artifacts
        find . -name "*.json" -o -name "*.md" | grep -E "(analysis|optimization|decomposition)" | while read file; do
          if [[ -f "$file" ]]; then
            cp "$file" .github/optimization/
          fi
        done
        
        # Create optimization summary
        cat > .github/optimization/README.md << 'EOF'
        # NovaMind AI Code Optimization Results
        
        This directory contains AI-powered code optimization analysis and recommendations for the NovaMind project.
        
        ## Files
        - `codebase_analysis.json`: Detailed codebase analysis
        - `ai_optimization_plan.json`: Azure AI recommendations
        - `optimization_report.md`: Human-readable optimization report
        - `decomposition_plans.json`: Automated decomposition suggestions
        - `decomposition_guide.md`: Step-by-step implementation guide
        
        ## Implementation
        1. Review the optimization report
        2. Follow the decomposition guide
        3. Implement suggested changes systematically
        4. Test thoroughly after each change
        
        Generated by NovaMind AI Optimization Pipeline ðŸ¤–
        EOF
        
    - name: Commit optimization results
      run: |
        git add .github/optimization/
        git commit -m "ðŸ¤– AI Code Optimization Analysis
        
        - Azure AI-powered codebase analysis
        - Automated decomposition recommendations  
        - Performance optimization suggestions
        - Apple coding standards compliance plan
        
        Generated by: NovaMind AI Optimization Pipeline
        Timestamp: $(date)
        Optimization Type: ${{ github.event.inputs.optimization_type }}"
        
    - name: Push optimization branch
      run: |
        git push origin "$BRANCH_NAME"
        
    - name: Create Pull Request
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Create PR with GitHub CLI
        gh pr create \
          --title "ðŸ¤– AI-Powered Code Optimization: ${{ github.event.inputs.optimization_type }}" \
          --body "## AI Code Optimization Results
          
        This PR contains AI-powered optimization analysis and recommendations for NovaMind.
        
        ### Analysis Summary
        - **Optimization Type**: ${{ github.event.inputs.optimization_type }}
        - **AI Engine**: Azure OpenAI GPT-4
        - **Analysis Date**: $(date)
        - **Files Analyzed**: All Swift files in the project
        
        ### Key Features
        ðŸ§  **Azure AI Analysis**: Advanced code pattern recognition
        ðŸ“Š **Complexity Metrics**: Detailed file and function analysis  
        ðŸŽ¯ **Apple Compliance**: Swift coding standards validation
        âš¡ **Performance Insights**: Optimization opportunities identified
        ðŸ”§ **Automated Decomposition**: Ready-to-implement file splits
        
        ### What's Included
        - Comprehensive codebase analysis
        - AI-generated optimization recommendations
        - Automated decomposition plans
        - Step-by-step implementation guides
        - Performance improvement suggestions
        
        ### Next Steps
        1. ðŸ“‹ Review optimization report in \`.github/optimization/\`
        2. ðŸŽ¯ Prioritize recommendations by impact
        3. ðŸ”§ Implement decomposition plans systematically
        4. âœ… Test changes thoroughly
        5. ðŸ“ˆ Monitor performance improvements
        
        ### Benefits
        - âœ… Apple coding standards compliance
        - âœ… Improved maintainability
        - âœ… Better performance
        - âœ… Enhanced readability
        - âœ… Reduced cognitive load
        
        *Powered by Azure AI and GitHub Enterprise* ðŸš€" \
          --head "$BRANCH_NAME" \
          --base main

  optimization-summary:
    name: Optimization Summary
    runs-on: ubuntu-latest
    needs: [create-optimization-pr]
    if: always()
    
    steps:
    - name: Generate Summary
      run: |
        echo "ðŸ¤– NovaMind AI Code Optimization Complete!"
        echo "================================================"
        echo ""
        echo "âœ… Azure AI analysis completed"
        echo "âœ… Automated decomposition plans generated"
        echo "âœ… Optimization PR created"
        echo ""
        echo "ðŸŽ¯ Next Steps:"
        echo "1. Review the generated PR"
        echo "2. Implement recommended optimizations"
        echo "3. Monitor performance improvements"
        echo ""
        echo "ðŸ’¡ This pipeline uses your Azure credits efficiently"
        echo "   and GitHub Enterprise features for maximum value!"
